{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svd.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeaOhrviMZK",
        "colab_type": "text"
      },
      "source": [
        "# SVD model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYq4XhehnAaV",
        "colab_type": "text"
      },
      "source": [
        "### Set dataset and current working dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RiQEUmYGktp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Brycexu727/movielens-dataset.git\n",
        "%cd movielens-dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeMMGvKCm8uE",
        "colab_type": "text"
      },
      "source": [
        "### Import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoiNv01emSys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4YRuirtm2li",
        "colab_type": "text"
      },
      "source": [
        "### Set SVD parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl6LURkGl8Ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_set_name = \"ml-100k\"\n",
        "#data_set_name = \"ml-1m\"\n",
        "\n",
        "train_test_ratio = 0.2 # 20% data for testing, 80% for training\n",
        "#train_test_ratio = 0.5 # 50% data for testing, 50% for training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iojMbGrrGprY",
        "colab_type": "code",
        "outputId": "fd0eddac-00ee-4c21-bad0-3acc894fae22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "class SVD:\n",
        "  def __init__(\n",
        "      self, \n",
        "      hidden_feature=20, \n",
        "      batch_size=100, \n",
        "      learning_rate = 0.001,\n",
        "      epoch_size = 20\n",
        "\n",
        "  ):\n",
        "    self.hidden_feature = hidden_feature\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epoch_size = epoch_size\n",
        "    \n",
        "    self.hd = tf.placeholder\n",
        "    self.rd = tf.random_uniform\n",
        "    self.var = tf.Variable\n",
        "    self.zero = tf.zeros\n",
        "    self.ebd = tf.nn.embedding_lookup\n",
        "    self.reg = tf.contrib.layers.apply_regularization\n",
        "    self.mul = tf.multiply\n",
        "    self.reduce = tf.reduce_sum\n",
        "    self.t = tf.train.AdamOptimizer\n",
        "    self.mse = tf.losses.mean_squared_error\n",
        "    self.l2 = tf.contrib.layers.l2_regularizer\n",
        "    self.init = tf.compat.v1.global_variables_initializer\n",
        "  def train(\n",
        "      self, \n",
        "      train_data, \n",
        "      test_data, \n",
        "      mean_rating, \n",
        "      user_num, \n",
        "      movie_num, \n",
        "  ):\n",
        "\n",
        "    \n",
        "    userId = self.hd(tf.int32, [self.batch_size])\n",
        "    movieId = self.hd(tf.int32, [self.batch_size])\n",
        "    rate = self.hd(tf.float32, [self.batch_size, 1])\n",
        "    \n",
        "    user_ebd = self.var(self.rd([user_num+1, self.hidden_feature],0,0.3),\n",
        "                         trainable = True)\n",
        "    movie_ebd = self.var(self.rd([movie_num+1, self.hidden_feature],0,0.3),\n",
        "                          trainable = True)\n",
        "    user_input = self.ebd(user_ebd, userId)\n",
        "    movie_input = self.ebd(movie_ebd, movieId)\n",
        "    \n",
        "    user_bias_ebd = self.var(self.zero([user_num+1,1]), \n",
        "                        trainable = True)\n",
        "    movie_bias_ebd = self.var(self.zero([movie_num+1,1]), \n",
        "                         trainable = True)\n",
        "    user_bias = self.ebd(user_bias_ebd, userId)\n",
        "    movie_bias = self.ebd(movie_bias_ebd, movieId)\n",
        "\n",
        "    r1 = self.l2(0.002)\n",
        "    lamda1 = self.reg(r1,[user_input, movie_input])\n",
        "    r2 = self.l2(0.002)\n",
        "    lamda2 = self.reg(r2,[user_bias, movie_bias])\n",
        "\n",
        "    product = self.reduce(self.mul(user_input, movie_input), axis=1, keep_dims=True)\n",
        "\n",
        "    loss = self.mse(rate, product + user_bias + movie_bias + mean_rating) + lamda1 + lamda2\n",
        "    train = self.t(self.learning_rate).minimize(loss)\n",
        "    sess = tf.Session()\n",
        "    sess.run(self.init())\n",
        "    print('\\nSVD training...')\n",
        "    for epoch in range(self.epoch_size):\n",
        "      errs = []\n",
        "      for start, end in zip(\n",
        "          range(0, len(train_data), self.batch_size), \n",
        "          range(self.batch_size, len(train_data), self.batch_size)\n",
        "      ):\n",
        "        tmp, tmpp = sess.run(\n",
        "            [loss, train], \n",
        "            feed_dict = {\n",
        "                userId: train_data[start: end, 0], \n",
        "                movieId: train_data[start: end,1 ], \n",
        "                rate: train_data[start: end, 2].reshape(self.batch_size,1)\n",
        "            }\n",
        "        )\n",
        "        errs.append(tmp)\n",
        "      err = []\n",
        "      for start, end in zip(\n",
        "          range(0, len(test_data), self.batch_size), \n",
        "          range(self.batch_size, len(test_data), self.batch_size)\n",
        "      ):\n",
        "        res = sess.run(\n",
        "            product + user_bias + movie_bias + mean_rating, \n",
        "            feed_dict = {\n",
        "                userId: test_data[start: end, 0], \n",
        "                movieId: test_data[start: end, 1], \n",
        "                rate: test_data[start: end, 2].reshape(self.batch_size,1)\n",
        "            }\n",
        "        )\n",
        "        ground_truth = test_data[start:end, 2].reshape(self.batch_size,1)\n",
        "        tmp2 = []\n",
        "        for item in res:\n",
        "          tmp2.append([min(max(item[0],1),5)])\n",
        "        err.append((ground_truth - np.array(tmp2))*(ground_truth - np.array(tmp2)))\n",
        "      print('Epoch ' + str(epoch+1) + '/' + str(self.epoch_size))\n",
        "      print('Test RMSE: %.3f' % (np.sqrt(np.mean(err))))\n",
        "    sess.close()\n",
        "\n",
        "def data_process_svd(dataset_name, train_test_ratio=0.1):\n",
        "  data_frame_title = ['userId', 'movieId', 'rating', 'timestamp']\n",
        "  if dataset_name == \"ml-100k\":\n",
        "    data_frame = pd.read_csv(\n",
        "        './ml-100k/u.data', \n",
        "        sep ='\\t', \n",
        "        names=data_frame_title\n",
        "    )\n",
        "    user_num = data_frame.userId.unique().shape[0]\n",
        "    movie_num = data_frame.movieId.unique().shape[0]\n",
        "  elif dataset_name == \"ml-1m\":\n",
        "    data_frame = pd.read_csv(\n",
        "        './ml-1m/ratings.dat', \n",
        "        sep='::', \n",
        "        names=data_frame_title\n",
        "    )\n",
        "    user_num = max(data_frame.userId)\n",
        "    movie_num = max(data_frame.movieId)\n",
        "  else:\n",
        "    print(\"unknown data set!\")\n",
        "  \n",
        "  train_data, test_data = train_test_split(\n",
        "      data_frame, \n",
        "      test_size=train_test_ratio\n",
        "  )\n",
        "  train_data = np.array(train_data)\n",
        "  mean_rating = np.mean(train_data[:,2])\n",
        "  return train_data, np.array(test_data), mean_rating, user_num, movie_num\n",
        "\n",
        "train_data, test_data, mean_rating, user_num, movie_num\\\n",
        "= data_process_svd(data_set_name, train_test_ratio)\n",
        "\n",
        "s = SVD()\n",
        "s.train(train_data, test_data, mean_rating, user_num, movie_num)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0811 13:44:53.857661 140027953194880 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0811 13:44:53.908244 140027953194880 deprecation.py:506] From <ipython-input-5-b5d52763a720>:60: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0811 13:44:53.929272 140027953194880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "SVD training...\n",
            "Epoch 1/20\n",
            "Test RMSE: 1.033\n",
            "Epoch 2/20\n",
            "Test RMSE: 0.998\n",
            "Epoch 3/20\n",
            "Test RMSE: 0.981\n",
            "Epoch 4/20\n",
            "Test RMSE: 0.970\n",
            "Epoch 5/20\n",
            "Test RMSE: 0.961\n",
            "Epoch 6/20\n",
            "Test RMSE: 0.953\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}